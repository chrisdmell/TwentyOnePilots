{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[questions 1](https://intellipaat.com/blog/interview-question/apache-spark-interview-questions/)\n",
    "1. What is SparkContext in Apache Spark?\n",
    ">SparkContext is the entry point of Spark functionality. The most important step of any Spark driver application is to generate SparkContext. It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The resource manager can be one of these three- Spark Standalone, YARN, Apache Mesos.\n",
    "> It connects with the spark exucution environment.\n",
    "2. Functions of SparkContext? \n",
    ">1. Get current status of application\n",
    ">2. Cancel a job\n",
    ">3. Cancel a stage\n",
    ">4. Unpersist RDD\n",
    ">5. Access persisted RDD\n",
    ">6. Running job synchronously\n",
    ">7. Running job asynchronously\n",
    "3. What is persist and cache?\n",
    "> cache stores in memory, persists only works on dataframe API -- confirm answer\n",
    "4. What is narrow transformation/wide?\n",
    "> narrow tranformation is when u do a where statement, from diferent executors, the data is filtered and then aggreagation is done on it and sent to the cluster manager.\n",
    "> Wide transformation is when you say, which total population of the city with the highest something, then there is shuffle here. Spark shuffles data between the executors to find the aggregations. Here all the workers needs to work and report.\n",
    "5. What is the default partion in writing after shuffle? how to change it?\n",
    "> 200 \n",
    "> SET spark.sql.shuffle.partion()\n",
    "6. What are actions and transformations?\n",
    "> transformations - does something on RDD and returns to the same like map filter. \n",
    "> actions - gives output to local machine,\n",
    "7. \n",
    ">In Apache Spark, the central coordinator is called the driver. When you enter your code in spark, SparkContext in the driver program creates the job when we call an Action. This job submits to DAG Scheduler which creates the operator graph and then submits it to task Scheduler. Task Scheduler launches the task via cluster manager. Thus, with the help of a cluster manager, a Spark Application launch on a set of machines.\n",
    "> Apache Spark SparkContect -> job -> DAG scheduler - > creates a operator graph and submits -> task schedular -> Task schedular lauches this task via cluster manager. \n",
    "8. Are RDD cache d automatically? \n",
    ">Sometimes, yes. The RDDs are cached automatically in cases of a shuffle.\n",
    "9. Map-reduce vs spark? <br>\n",
    "\n",
    "| Metric | Apache Spark | Hadoop |\n",
    "| --- | --- | --- |\n",
    "| Data Processing | Batch processing and real time data processing | only for batch processing |\n",
    "| Speed | 100 father in memory 10 faster in disk | slower than spark due to i/o(input output) disk latency|\n",
    "| Scalability | 1000 nodes per cluster  | 1000 nodes per cluster |\n",
    "| ML |  in-build library | apache mahout used |\n",
    "| Schedular | has its own | dependant on outside |\n",
    "| fault tolerant | | |\n",
    "| duplication elimination | sparks process each records exactly once | doesnt support this |\n",
    "| letency | quick | higher latency due to i/o disk |\n",
    "| sql | though hive | spark SQL|\n",
    "\n",
    "10. Define RDD.\n",
    ">RDD is the acronym for Resilient Distribution Datasetsâ€”a fault-tolerant collection of operational elements that run in parallel. The partitioned data in an RDD is immutable and distributed. There are primarily two types of RDDs:\n",
    ">\n",
    ">Parallelized collections: The existing RDDs running in parallel with one another\n",
    ">Hadoop datasets: Those performing a function on each file record in HDFS or any other storage system\n",
    "11. What are partitions?\n",
    "> Smaller and logical divisions that speed up data processing\n",
    "12. Define the functions of Spark Core.\n",
    "> Serving as the base engine, Spark Core performs various important functions like memory management, monitoring jobs, providing fault-tolerance, job scheduling, and interaction with storage systems.\n",
    "13. What is Spark Executor?\n",
    "> When SparkContext connects to Cluster Manager, it acquires an executor on the nodes in the cluster. Executors are Spark processes that run computations and store data on worker nodes. The final tasks by SparkContext are transferred to executors for their execution.\n",
    "14. Illustrate some demerits of using Spark.\n",
    ">Since Spark utilizes more storage space when compared to Hadoop and MapReduce, there might arise certain problems. Developers need to be careful while running their applications on Spark. To resolve the issue, they can think of distributing the workload over multiple clusters, instead of running everything on a single node.\n",
    "15. What is Spark Driver?\n",
    "> Spark driver is the program that runs on the master node of a machine and declares transformations and actions on data RDDs. In simple terms, a driver in Spark creates SparkContext, connected to a given Spark Master. It also delivers RDD graphs to Master, where the standalone Cluster Manager runs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
