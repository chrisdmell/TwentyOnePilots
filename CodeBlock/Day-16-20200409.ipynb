{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark\n",
    "Basic EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PySpark module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName(\"test\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# What version of Spark?\n",
    "##print(pyspark.__version__)\n",
    "\n",
    "# Terminate the cluster\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://IABLR-LT025.actived-impactanalytics.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eff39906d8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spin up a spark session using all the cores\n",
    "Builder lets us do \n",
    "1. specify the location of the master node;\n",
    "2. name the application (optional); and\n",
    "3. retrieve an existing SparkSession or, if there is none, create a new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPARK 2.0.0 onwards\n",
    "\n",
    "__SparkSession__ provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with Dataframe and Dataset APIs. All the functionality available with sparkContext are also available in sparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 14430 records.\n",
      "+--------------+-----+------+-------------------+---------+------+---------+\n",
      "|Country_Region|  Lat|  Long|           date_new|confirmed|deaths|recovered|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+\n",
      "|            US|37.09|-95.71|2020-01-22 00:00:00|      1.0|     0|     null|\n",
      "|            US|37.09|-95.71|2020-01-23 00:00:00|      1.0|     0|     null|\n",
      "|            US|37.09|-95.71|2020-01-24 00:00:00|      2.0|     0|     null|\n",
      "|            US|37.09|-95.71|2020-01-25 00:00:00|      2.0|     0|     null|\n",
      "|            US|37.09|-95.71|2020-01-26 00:00:00|      3.0|     0|     null|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('Country_Region', 'string'),\n",
       " ('Lat', 'double'),\n",
       " ('Long', 'double'),\n",
       " ('date_new', 'timestamp'),\n",
       " ('confirmed', 'double'),\n",
       " ('deaths', 'int'),\n",
       " ('recovered', 'double')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "\n",
    "# scheme\n",
    "scheme_ = StructType([\n",
    "    StructField(\"mon\", StringType()),\n",
    "    StructField(\"dom\", IntegerType()),\n",
    "    StructField(\"dow\", IntegerType()),\n",
    "    StructField(\"org\", StringType()),\n",
    "    StructField(\"mile\", IntegerType()),\n",
    "    StructField(\"carrier\", StringType()),\n",
    "    StructField(\"depart\", StringType()),\n",
    "    StructField(\"duration\", StringType()),\n",
    "    StructField(\"delay\", StringType())\n",
    "    ])\n",
    "#\n",
    "covid_df = spark.read.csv(\"tableau_covid_data.csv\", header = \"true\", inferSchema = 'true')\n",
    "\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % covid_df.count())\n",
    "\n",
    "# View the first five records\n",
    "display(covid_df.show(5))\n",
    "\n",
    "# Check column data types\n",
    "display(covid_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand big-data we need to understand the hardware concenpts, like how much time does it take to read a song in laptop, or how much time does it take to load calenders that is caches in your system. Or how long does it take for CPU to add two number? <br>\n",
    "_Memory_ is short term quick storage<br>\n",
    "_SSD(solid state drive)_ is long-term storage<br>\n",
    "_Network_ is connection between computers<br>\n",
    "<br>\n",
    "Memory(Memory (aka RAM) is known to be \"efficient, expensive, and ephemeral\". Operations in RAM are relatively fast compared to reading and writing from disk or moving data across a network. However, RAM is expensive, and data stored in RAM will get erased when a computer shuts down.) is expensive compared to storage, and once the data is loaded in memory, when shut down it is lost. So instead, we have them stored in SSD. <br>\n",
    "A __cluster__ or __distributed system__ is long term storage and is collection of _nodes_, each hardware is called a _node_.  <br>\n",
    "<br>\n",
    "Shuffling : - Moving data back and forth between nodes.   Transferring data across a network, ie between computers, is the biggest bottleneck when working with big data. One of the advantages of Spark is that it only shuffles data between computers when it absolutely has to.<br>\n",
    "Thrashing : - The process of the CPU where it moves the IO data from memory to disk and wise versa. <br><br>\n",
    "\n",
    "Hadoop is slow, because the results are saved to disk, and saving backing back to disc from CPU, does take long time to Hadoop is slower than SPARK. Hadoop has four main components \n",
    "1. HDFS(hadoop distributed file system),<br> \n",
    "2. MapReduce function,<br> \n",
    "3. Resource Manager YARN (yet another resource negotiator),<br> \n",
    "4. Utilities on top of this like spark-pig, HIVE etc. \n",
    "<br><br>\n",
    "How is Spark related to Hadoop?<br>\n",
    "Spark, which is the main focus of this course, is another big data framework. Spark contains libraries for data analysis, machine learning, graph analysis, and streaming live data. Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible.\n",
    "\n",
    "The Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as Amazon S3.\n",
    "<br><br>\n",
    "The hadoop system has say these processes, the data is in HDFS, then  loaded to the storage machines, or partitions, then shuffles, so that the simiar key-value paris are closer than further apart. The reduce step combines the values and gives the final output for the same. In map reduce, data is organized into (key, value) pairs. The shuffle step finds all of the data across the clusters that have the same key. And all of those data points with that key are brought into the same network node for further analysis.\n",
    "<br><br>\n",
    "Spark can be run on local, but its not distributed computing.<br>\n",
    "For it to be distributed, we need cluster manager, Cluster mode can be _standalone_ or _YARN_ or _mesos_ <br>\n",
    "When we access spark on python, there is a __driver__ program, which helps to schedule tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Spark Driver?\n",
    "> Spark driver is the program that runs on the master node of a machine and declares transformations and actions on data RDDs. In simple terms, a driver in Spark creates SparkContext, connected to a given Spark Master. It also delivers RDD graphs to Master, where the standalone Cluster Manager runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[20] at RDD at PythonRDD.scala:53"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['scala',\n",
       " 'java',\n",
       " 'hadoop',\n",
       " 'spark',\n",
       " 'akka',\n",
       " 'spark vs hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark and spark']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = spark.sparkContext.parallelize (  ## this creates a RDD - in my local. \n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "\n",
    "def convert_title_to_upper(title):\n",
    "    '''\n",
    "    This is ideally a method and not a pure function\n",
    "    As upper() function is used\n",
    "    '''\n",
    "    return title.upper()\n",
    "\n",
    "display(words.map(convert_title_to_upper))\n",
    "\n",
    "display(words.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.93 s ± 371 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "words.map(lambda x:x.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12 ms ± 71.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "words.map(lambda x:x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The massive difference in time is due the lazy execution of spark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting and selecting the columns needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lat', 'Long', 'Confirmed']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+------+-------------------+---------+------+---------+\n",
      "|Country_Region|  Lat|  Long|           date_new|confirmed|deaths|recovered|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+\n",
      "|            US|37.09|-95.71|2020-01-22 00:00:00|      1.0|     0|     null|\n",
      "|            US|37.09|-95.71|2020-01-23 00:00:00|      1.0|     0|     null|\n",
      "|            US|37.09|-95.71|2020-01-24 00:00:00|      2.0|     0|     null|\n",
      "|            US|37.09|-95.71|2020-01-25 00:00:00|      2.0|     0|     null|\n",
      "|            US|37.09|-95.71|2020-01-26 00:00:00|      3.0|     0|     null|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Country_Region',\n",
       " 'Lat',\n",
       " 'Long',\n",
       " 'date_new',\n",
       " 'confirmed',\n",
       " 'deaths',\n",
       " 'recovered']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|  Lat|  Long|\n",
      "+-----+------+\n",
      "|37.09|-95.71|\n",
      "|37.09|-95.71|\n",
      "|37.09|-95.71|\n",
      "|37.09|-95.71|\n",
      "|37.09|-95.71|\n",
      "+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_string = 'Lat,Long,Confirmed'\n",
    "my_string_list = my_string.split(',')\n",
    "display(my_string_list)\n",
    "\n",
    "display(covid_df.show(5))\n",
    "display(covid_df.columns)\n",
    "\n",
    "covid_df.select([x for x in covid_df.columns if x in my_string_list]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns using ```withColumnRenamed```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------+-------------------+---------+------+---------+\n",
      "|Country_Region|Latitude|  Long|           date_new|confirmed|deaths|recovered|\n",
      "+--------------+--------+------+-------------------+---------+------+---------+\n",
      "|            US|   37.09|-95.71|2020-01-22 00:00:00|      1.0|     0|     null|\n",
      "|            US|   37.09|-95.71|2020-01-23 00:00:00|      1.0|     0|     null|\n",
      "|            US|   37.09|-95.71|2020-01-24 00:00:00|      2.0|     0|     null|\n",
      "|            US|   37.09|-95.71|2020-01-25 00:00:00|      2.0|     0|     null|\n",
      "|            US|   37.09|-95.71|2020-01-26 00:00:00|      3.0|     0|     null|\n",
      "+--------------+--------+------+-------------------+---------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df.withColumnRenamed('Lat', 'Latitude').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change scheme using ```withColumns(new_col_name, to_do)``` and replace the old column with new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country_Region: string (nullable = true)\n",
      " |-- Lat: float (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- date_new: timestamp (nullable = true)\n",
      " |-- confirmed: double (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      " |-- recovered: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "## withColumn(new_Columns, columns_to_change)\n",
    "changedTypedf = covid_df.withColumn(\"Lat\", covid_df[\"Lat\"].cast(FloatType()))\n",
    "changedTypedf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaning to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date_new: timestamp (nullable = true)\n",
      " |-- trans_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Row(Date_new=datetime.datetime(2020, 1, 22, 0, 0), trans_date='2020-01-22 00:00:00'),\n",
       " Row(Date_new=datetime.datetime(2020, 1, 23, 0, 0), trans_date='2020-01-23 00:00:00'),\n",
       " Row(Date_new=datetime.datetime(2020, 1, 24, 0, 0), trans_date='2020-01-24 00:00:00')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "changedTypedf = covid_df.select(\"Date_new\",\n",
    "                              from_unixtime(unix_timestamp('Date_new', 'dd-MM-yyyy')).alias('trans_date'))\n",
    "\n",
    "display(changedTypedf.printSchema())\n",
    "display(changedTypedf.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+------+-------------------+---------+------+---------+----------+\n",
      "|Country_Region|  Lat|  Long|           date_new|confirmed|deaths|recovered|  date_new|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+----------+\n",
      "|            US|37.09|-95.71|2020-01-22 00:00:00|      1.0|     0|     null|2020-01-22|\n",
      "|            US|37.09|-95.71|2020-01-23 00:00:00|      1.0|     0|     null|2020-01-23|\n",
      "|            US|37.09|-95.71|2020-01-24 00:00:00|      2.0|     0|     null|2020-01-24|\n",
      "|            US|37.09|-95.71|2020-01-25 00:00:00|      2.0|     0|     null|2020-01-25|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country_Region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- date_new: timestamp (nullable = true)\n",
      " |-- confirmed: double (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      " |-- recovered: double (nullable = true)\n",
      " |-- date_new: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "test_003 = covid_df.select('*',to_date(covid_df.date_new, 'dd-MM-yyyy').alias('date_new'))\n",
    "display(test_003.show(4))\n",
    "display(test_003.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data manipulatiob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+------+-------------------+---------+------+---------+-----------+\n",
      "|Country_Region|  Lat|  Long|           date_new|confirmed|deaths|recovered|len_country|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+-----------+\n",
      "|            US|37.09|-95.71|2020-01-22 00:00:00|      1.0|     0|     null|    -9571.0|\n",
      "|            US|37.09|-95.71|2020-01-23 00:00:00|      1.0|     0|     null|    -9571.0|\n",
      "|            US|37.09|-95.71|2020-01-24 00:00:00|      2.0|     0|     null|    -9571.0|\n",
      "|            US|37.09|-95.71|2020-01-25 00:00:00|      2.0|     0|     null|    -9571.0|\n",
      "|            US|37.09|-95.71|2020-01-26 00:00:00|      3.0|     0|     null|    -9571.0|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "covid_df.withColumn('len_country', col('Long')*100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+------+-------------------+---------+------+---------+-------+\n",
      "|Country_Region|  Lat|  Long|           date_new|confirmed|deaths|recovered|   fare|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+-------+\n",
      "|            US|37.09|-95.71|2020-01-22 00:00:00|      1.0|     0|     null|-9571.0|\n",
      "|            US|37.09|-95.71|2020-01-23 00:00:00|      1.0|     0|     null|-9571.0|\n",
      "|            US|37.09|-95.71|2020-01-24 00:00:00|      2.0|     0|     null|-9571.0|\n",
      "|            US|37.09|-95.71|2020-01-25 00:00:00|      2.0|     0|     null|-9571.0|\n",
      "|            US|37.09|-95.71|2020-01-26 00:00:00|      3.0|     0|     null|-9571.0|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df.select('*',(col('Long')*100).alias('fare')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         confirmed|\n",
      "+-------+------------------+\n",
      "|  count|             14430|\n",
      "|   mean|1650.4255024255024|\n",
      "| stddev|14024.006493671819|\n",
      "|    min|               0.0|\n",
      "|    max|          429052.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df.describe(\"confirmed\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|     Country_Region|\n",
      "+-------------------+\n",
      "|                 US|\n",
      "|        afghanistan|\n",
      "|            albania|\n",
      "|            algeria|\n",
      "|            andorra|\n",
      "|             angola|\n",
      "|antigua and barbuda|\n",
      "|          argentina|\n",
      "|            armenia|\n",
      "|          australia|\n",
      "|            austria|\n",
      "|         azerbaijan|\n",
      "|            bahamas|\n",
      "|            bahrain|\n",
      "|         bangladesh|\n",
      "|           barbados|\n",
      "|            belarus|\n",
      "|            belgium|\n",
      "|             belize|\n",
      "|              benin|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df.select('Country_Region').dropDuplicates().sort('Country_Region').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.select('*').where(covid_df.Country_Region == \"Bangladesh\").collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```%%timeit``` to show the idea of lazy evaluation in ```Spark``` <br>\n",
    "1. 500ms to actually execute\n",
    "2. 22.2ms for no evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ms ± 51.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "covid_df.select('*').where(covid_df.Country_Region == \"bangladesh\").collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.2 ms ± 1.22 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "covid_df.select('*').where(covid_df.Country_Region == \"bangladesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----+-------------------+---------+------+---------+\n",
      "|Country_Region|    Lat| Long|           date_new|confirmed|deaths|recovered|\n",
      "+--------------+-------+-----+-------------------+---------+------+---------+\n",
      "|         nepal|28.1667|84.25|2020-01-22 00:00:00|      0.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-23 00:00:00|      0.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-24 00:00:00|      0.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-25 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-26 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-27 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-28 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-29 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-30 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-01-31 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-01 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-02 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-03 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-04 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-05 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-06 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-07 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-08 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-09 00:00:00|      1.0|     0|      0.0|\n",
      "|         nepal|28.1667|84.25|2020-02-10 00:00:00|      1.0|     0|      0.0|\n",
      "+--------------+-------+-----+-------------------+---------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df.filter(covid_df[\"Country_Region\"] == \"nepal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Defined Functions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+------+-------------------+---------+------+---------+-------+\n",
      "|Country_Region|  Lat|  Long|           date_new|confirmed|deaths|recovered|  Long_|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+-------+\n",
      "|            US|37.09|-95.71|2020-01-22 00:00:00|      1.0|     0|     null|-191.42|\n",
      "|            US|37.09|-95.71|2020-01-23 00:00:00|      1.0|     0|     null|-191.42|\n",
      "|            US|37.09|-95.71|2020-01-24 00:00:00|      2.0|     0|     null|-191.42|\n",
      "+--------------+-----+------+-------------------+---------+------+---------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf  ## user defined functions\n",
    "some_function = udf(lambda x : x+x)\n",
    "\n",
    "new = covid_df.withColumn('Long_', some_function(covid_df.Long))\n",
    "\n",
    "new.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>date_new</th>\n",
       "      <th>confirmed</th>\n",
       "      <th>deaths</th>\n",
       "      <th>recovered</th>\n",
       "      <th>Long_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>37.09</td>\n",
       "      <td>-95.71</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-191.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>37.09</td>\n",
       "      <td>-95.71</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-191.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>37.09</td>\n",
       "      <td>-95.71</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-191.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>37.09</td>\n",
       "      <td>-95.71</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-191.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>37.09</td>\n",
       "      <td>-95.71</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-191.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country_Region    Lat   Long   date_new  confirmed  deaths  recovered  \\\n",
       "0             US  37.09 -95.71 2020-01-22        1.0       0        NaN   \n",
       "1             US  37.09 -95.71 2020-01-23        1.0       0        NaN   \n",
       "2             US  37.09 -95.71 2020-01-24        2.0       0        NaN   \n",
       "3             US  37.09 -95.71 2020-01-25        2.0       0        NaN   \n",
       "4             US  37.09 -95.71 2020-01-26        3.0       0        NaN   \n",
       "\n",
       "     Long_  \n",
       "0  -191.42  \n",
       "1  -191.42  \n",
       "2  -191.42  \n",
       "3  -191.42  \n",
       "4  -191.42  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_spark_to_pandas = new.toPandas()\n",
    "from_spark_to_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Spark window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-------+-------------------+---------+------+---------+-----------------+\n",
      "| Country_Region|    Lat|   Long|           date_new|confirmed|deaths|recovered|avg_death_country|\n",
      "+---------------+-------+-------+-------------------+---------+------+---------+-----------------+\n",
      "|north macedonia|41.6086|21.7453|2020-01-22 00:00:00|      0.0|     0|      0.0|              187|\n",
      "|north macedonia|41.6086|21.7453|2020-01-23 00:00:00|      0.0|     0|      0.0|              187|\n",
      "|north macedonia|41.6086|21.7453|2020-01-24 00:00:00|      0.0|     0|      0.0|              187|\n",
      "|north macedonia|41.6086|21.7453|2020-01-25 00:00:00|      0.0|     0|      0.0|              187|\n",
      "|north macedonia|41.6086|21.7453|2020-01-26 00:00:00|      0.0|     0|      0.0|              187|\n",
      "|north macedonia|41.6086|21.7453|2020-01-27 00:00:00|      0.0|     0|      0.0|              187|\n",
      "+---------------+-------+-------+-------------------+---------+------+---------+-----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "byCountryDeath = Window.partitionBy('Country_Region')\n",
    "\n",
    "covid_df_partitionBy = covid_df.withColumn(\"avg_death_country\", sum('deaths').over (byCountryDeath))\n",
    "covid_df_partitionBy.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|Country_Region|avg_death_country|\n",
      "+--------------+-----------------+\n",
      "|         italy|           240848|\n",
      "|         china|           172428|\n",
      "|         spain|           152278|\n",
      "|            US|            92246|\n",
      "|            us|            92194|\n",
      "|        france|            83133|\n",
      "|          iran|            62520|\n",
      "|united kingdom|            45946|\n",
      "|   netherlands|            19507|\n",
      "|       germany|            16381|\n",
      "|       belgium|            14764|\n",
      "|   switzerland|             8043|\n",
      "|        turkey|             5295|\n",
      "|        brazil|             4899|\n",
      "|        sweden|             4379|\n",
      "|  korea, south|             4135|\n",
      "|      portugal|             3031|\n",
      "|        canada|             2589|\n",
      "|     indonesia|             2556|\n",
      "|       austria|             2224|\n",
      "+--------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "covid_df_partitionBy.select([\"Country_Region\", \"avg_death_country\"]).drop_duplicates().sort([\"avg_death_country\"], ascending = False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[questions 1](https://intellipaat.com/blog/interview-question/apache-spark-interview-questions/)\n",
    "1. What is SparkContext in Apache Spark?\n",
    ">SparkContext is the entry point of Spark functionality. The most important step of any Spark driver application is to generate SparkContext. It allows your Spark Application to access Spark Cluster with the help of Resource Manager. The resource manager can be one of these three- Spark Standalone, YARN, Apache Mesos.\n",
    "> It connects with the spark exucution environment.\n",
    "2. Functions of SparkContext? \n",
    ">1. Get current status of application\n",
    ">2. Cancel a job\n",
    ">3. Cancel a stage\n",
    ">4. Unpersist RDD\n",
    ">5. Access persisted RDD\n",
    ">6. Running job synchronously\n",
    ">7. Running job asynchronously\n",
    "3. What is persist and cache?\n",
    "> cache stores in memory, persists only works on dataframe API -- confirm answer\n",
    "4. What is narrow transformation/wide?\n",
    "> narrow tranformation is when u do a where statement, from diferent executors, the data is filtered and then aggreagation is done on it and sent to the cluster manager.\n",
    "> Wide transformation is when you say, which total population of the city with the highest something, then there is shuffle here. Spark shuffles data between the executors to find the aggregations. Here all the workers needs to work and report.\n",
    "5. What is the default partion in writing after shuffle? how to change it?\n",
    "> 200 \n",
    "> SET spark.sql.shuffle.partion()\n",
    "6. What are actions and transformations?\n",
    "> transformations - does something on RDD and returns to the same like map filter. \n",
    "> actions - gives output to local machine,\n",
    "7. \n",
    ">In Apache Spark, the central coordinator is called the driver. When you enter your code in spark, SparkContext in the driver program creates the job when we call an Action. This job submits to DAG Scheduler which creates the operator graph and then submits it to task Scheduler. Task Scheduler launches the task via cluster manager. Thus, with the help of a cluster manager, a Spark Application launch on a set of machines.\n",
    "> Apache Spark SparkContect -> job -> DAG scheduler - > creates a operator graph and submits -> task schedular -> Task schedular lauches this task via cluster manager. \n",
    "8. Are RDD cache d automatically? \n",
    ">Sometimes, yes. The RDDs are cached automatically in cases of a shuffle.\n",
    "9. Map-reduce vs spark? <br>\n",
    "\n",
    "| Metric | Apache Spark | Hadoop |\n",
    "| --- | --- | --- |\n",
    "| Data Processing | Batch processing and real time data processing | only for batch processing |\n",
    "| Speed | 100 father in memory 10 faster in disk | slower than spark due to i/o(input output) disk latency|\n",
    "| Scalability | 1000 nodes per cluster  | 1000 nodes per cluster |\n",
    "| ML |  in-build library | apache mahout used |\n",
    "| Schedular | has its own | dependant on outside |\n",
    "| fault tolerant | | |\n",
    "| duplication elimination | sparks process each records exactly once | doesnt support this |\n",
    "| letency | quick | higher latency due to i/o disk |\n",
    "| sql | though hive | spark SQL|\n",
    "\n",
    "10. Define RDD.\n",
    ">RDD is the acronym for Resilient Distribution Datasets—a fault-tolerant collection of operational elements that run in parallel. The partitioned data in an RDD is immutable and distributed. There are primarily two types of RDDs:\n",
    ">\n",
    ">Parallelized collections: The existing RDDs running in parallel with one another\n",
    ">Hadoop datasets: Those performing a function on each file record in HDFS or any other storage system\n",
    "11. What are partitions?\n",
    "> Smaller and logical divisions that speed up data processing\n",
    "12. Define the functions of Spark Core.\n",
    "> Serving as the base engine, Spark Core performs various important functions like memory management, monitoring jobs, providing fault-tolerance, job scheduling, and interaction with storage systems.\n",
    "13. What is Spark Executor?\n",
    "> When SparkContext connects to Cluster Manager, it acquires an executor on the nodes in the cluster. Executors are Spark processes that run computations and store data on worker nodes. The final tasks by SparkContext are transferred to executors for their execution.\n",
    "14. Illustrate some demerits of using Spark.\n",
    ">Since Spark utilizes more storage space when compared to Hadoop and MapReduce, there might arise certain problems. Developers need to be careful while running their applications on Spark. To resolve the issue, they can think of distributing the workload over multiple clusters, instead of running everything on a single node.\n",
    "15. What is Spark Driver?\n",
    "> Spark driver is the program that runs on the master node of a machine and declares transformations and actions on data RDDs. In simple terms, a driver in Spark creates SparkContext, connected to a given Spark Master. It also delivers RDD graphs to Master, where the standalone Cluster Manager runs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
