{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticnet, special type of regularized Linear Regression.  The elastic-net selects variables like\n",
    "the lasso, and shrinks together the coefficients of correlated predictors like\n",
    "ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=1}^{p}(\\alpha|\\beta_j| + (1-\\alpha)\\beta_j^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 and L1 regularization differ in how they cope with correlated predictors: L2 will divide the coefficient loading equally among them whereas L1 will place all the loading on one of them while shrinking the others towards zero. Elastic Net combines the advantages of both: it tends to either select a group of correlated predictors in which case it puts equal loading on all of them, or it completely shrinks the group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the advantages of glmnet over other glm?\n",
    "> 1. It can set limit to the coefficients ( we set limit of promo coefficients, we use alpha = 1 i.e. lasso regression,  )\n",
    "> 2. Need to be linear relationship.\n",
    "> 3. High number of features and sparse data , glmnet gives good results \n",
    "> 4. \n",
    "2. Why glmnet over ols?\n",
    ">especially if I have a high number of features. Penalizing helps us to have a deeper understanding of the importance of each coefficient.\n",
    "3. Feature engineering?\n",
    "> weighted price, because there is variation in price throughout locations, year/month/week trend variables, sku_revenue_week/month variable for trend.\n",
    "4. Quantity models/ Quantity per store models?\n",
    "> Quantity models is used when we do not have inventory, because this varible is colinear with (860*(quantity/store_count), we kind of extrapolate this)  . When we do not have inventory then we will not know the store count in where the product was present even when it was not sold. So quantity/store will be linear with the \n",
    "5. Use of regularization?\n",
    "> We use regularization to tackle the problem of overfitting, there are two types [L1(lasso)](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when) this creates a spares matrix, the absolute co-eff term is added to the cost function, this leads to feature selection. L2(Ridge) squares of coeff\n",
    "6. How to choose learning rate (alpha)?\n",
    "> We can use the cost vs iteration grpah to choose the same, If alpha is too high overshoot the optimal point and if alpha is very less than the algorithm will take time to converge. \n",
    "7. How to choose the regularization parameter?\n",
    "> How much regularization is need that a tricky business, if very much then the model will have high bias - low variance and vice versa. using different lambda values then see what values ----------------------------IMP---------------------\n",
    "8. Why use regression instead of time series? can we?\n",
    "> Regression is not advised for forecasting coz we cannot extrapolate, the same. One consition for regression is the errors not to be auto-correaltion, and this is where time series comes in. \n",
    "9. Sum of residuls come to zero. Justify!!\n",
    "> Consider a line of data points ( straight line basically) where the slope is 0.5 and if we fit a regression line on it, which will exactly co -incide on it, here the error is zero. This is one basic idea behind regression, that the error term is zero, but in real world senario this doesn't happen. In the equation we are trying to minimise the error, so y value - the sum of intercept - bx is zero, which is equal to the error term. y = a + bx + error. \n",
    "10. How does multicollinearity affect regression?\n",
    "> This causes problems in interpretation of the model. The predictive capabaility of the model wont change. To deal with this we have VIF( variance inflation factor), the higher value meaning that there is multicollinearity in the data and we can remove the variation in the order specified, with the least important variables being removed first. \n",
    "11. Normal Form of linear regression equation? when should it be preferred?\n",
    "> the vector form \n",
    "12. Beta values come different for different subset of data?\n",
    "> Meaning the data is not homogeneoues, need to make clusters then model each cluster differently or use tree based models. \n",
    "13. Infinite # of estimates of regression co-efficients and the regression \n",
    "> This happens when there is multi-collinearity in the data and, hence there is no one value for a co-eff\n",
    "14. Adjsuted r2 and R2?\n",
    "> Adj R2 takes into consideration relevant variables only where p<0.5 R2 increase when we add variables. So for multiple linear regression we take Adj R2 ---- formula if needed. $$ R^2 = 1- [(1-R^2)(n-1)/(n-k-1)] $$\n",
    "> R2 is the measure of variance that the model can explain $$ 1-(ss_residuals/sst_total) $$\n",
    "> R2 adjusted is used for multiple regression, here we have a penalty term for (n-1)/(n-k-1) for each added variable\n",
    "15. Interpret residuals vs fitted curve\n",
    "> Residual vs predicted gives the variation of the residuals about the fitted line, there needs to coherant distribution(homoscedasticity - this can be checked )\n",
    "16. What is hetroscedasticity, consequences and how to overcome?\n",
    "> Sub population has differnt SD , can be found from residual vs fitted_values line( it ll be like a cone kinda graph), To overcome - log the data or \n",
    "17. What is VIF and how to calcualte the same?\n",
    ">variation inflation factor is like - for the independant variables, each variables is regressed against the rest, and if we get a high value for vif variables then there is multi collinearity.\n",
    "18. How do yo know if linear regression is suitable for any given data?\n",
    "> Two dimentional scattter plots,  SCATTER PLOTS.\n",
    "19. How is hypothesis testing used in LR?\n",
    "> Two ways, p-value<0.5 second, if the co-eff is significant, that depends on business sense as well\n",
    "20. Explain gradient descent wrt to linear regression?\n",
    ">  Optimization algorithm, to reduce the cost function, and at the optimal value get the estimators, its like hill descent'\n",
    "21. How do you interpret a linear regression model?\n",
    "> How much change in y is made due to one unit change in x1 keeping rest all constant.\n",
    "22. What is robust regression?\n",
    "> The model needs to have low bias and variance and fit well to unseen data. Outliers are a big problem to deal with this we have weighted linear regression.\n",
    "23. Weighted least squares?\n",
    "> ---------------- ANS----------------\n",
    "24. Graphs to observe before we fit the model?\n",
    "> check for trend, disttribution, skewness, box plots, histograms, \n",
    "25. What is generalized linear model?\n",
    "> 1. Is a derivative of OLS.\n",
    "> 2. GLM is flexible wrt to residuls, they need not be normally distributed.\n",
    "> 3. It generalized the linear regression by allowing the linear model to link to the target variable using the linking function. \n",
    "> 4. Model estimate is done using maximum likehood function\n",
    "> 5. Problem with OLs - . The problem with this kind of prediction model would imply a temperature drop of 10 degrees would lead to 1,000 fewer people visiting the beach, a beach whose expected attendance was 50 at a higher temperature would now be predicted to have the impossible attendance value of âˆ’950. SO for the output variable to be positive we can have GLM.\n",
    "26. ADVT vs DIS-ADVT  of glm and ols\n",
    ">1.Summary of advantages of GLMs over traditional (OLS) [regression](https://online.stat.psu.edu/stat504/node/216/)\n",
    ">We do not need to transform the response Y to have a normal distribution\n",
    ">The choice of link is separate from the choice of random component thus we have more flexibility in modeling\n",
    ">If the link produces additive effects, then we do not need constant variance.\n",
    ">The models are fitted via Maximum Likelihood estimation; thus optimal properties of the estimators.\n",
    ">All the inference tools and model checking that we will discuss for log-linear and logistic regression models apply for other GLMs too; e.g., Wald and Likelihood ratio tests, Deviance, Residuals, Confidence intervals, Overdispersion.\n",
    ">There is often one procedure in a software package to capture all the models listed above, e.g. PROC GENMOD in SAS or glm() in R, etc... with options to vary the three components.\n",
    ">2.But there are some limitations of GLMs too, such as,\n",
    ">Linear function, e.g. can have only a linear predictor in the systematic component\n",
    ">Responses must be independent .\n",
    "> our data was discrete and not continuos.\n",
    "27. What estimation does glm use?\n",
    ">It uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS) to estimate the parameters, and thus relies on large-sample approximations.\n",
    "28. Maximum likelihood estimation/ COst fucntion\n",
    "> ----------------- ANS ---------------------------\n",
    "29. Objective of simple regression\n",
    "> Objective\n",
    "> Model structure (e.g. variables, formula, equation)\n",
    "> Model assumptions\n",
    "> Parameter estimates and interpretation\n",
    "> Model fit (e.g. goodness-of-fit tests and statistics)\n",
    "> Model selection\n",
    "30. Explain Bias Variance Trade off?\n",
    "> This is related to business point of view, if the models needs to genralize to many other new data then the model needs to have low variance( which can mean high bias for unseen data). \n",
    "31. How can learning curve help create better model?\n",
    "> \n",
    "32. What are generalized linear models?\n",
    "> Generalized linear models are differnt from OLS in a way that, the basic assumptions of GLM. There neeed not be linear relationship between indendant and depandant varibles, the residuals need not be guassian/normally distributed, We can restrict the output variables to a certain considition, even the co-effcients can be done the same. OLS works on minimising ols but GLM works on MLE - using gradient ascent. It works well on sparse data. \n",
    "33. Likelihood\n",
    "> They will be small numbers less than one, The function that we will explore here is [Data Likehood](https://nbviewer.jupyter.org/github/twistedhardware/mltutorial/blob/master/notebooks/jupyter/2.%20Markdown%20%26%20LaTeX.ipynb) - \n",
    "$$l(w) = \\prod\\limits_{i=1}^{N} P(y_i|x_i, w) $$\n",
    "\n",
    "> Say for a [logistic regression](https://www.coursera.org/learn/ml-classification/lecture/Qjvns/data-likelihood) we want to predict +1 or -1, say for a given data point predict given y = +1 given x1 and parameters w. For the second data point y = +1 given x1 and parameters w . The likelihood function is the product of the probabilities of individual data points. \n",
    "> The elastinet penalty is controlled by $$\\alpha $$ (this gives the control between lasso or ridge) and the $$\\lambda$$ control the strength of the penalty. \n",
    "\n",
    "34. What happens in GLM when we have corellated groups and alpha is 0.5?\n",
    "> The models either picks the groups or leaves it. \n",
    "35. What to do to remove wild coreelations in glmnet?\n",
    "> choose an alpha that is close to 1.\n",
    "36. The mle is actually log likelihood explain?\n",
    ">[explain](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf)<br>\n",
    "> we have the probability density function for each y, when we multiply them all together we get the likelihood function,( this function is a funtion of parameter values (the parameters are the velues that we estimate)) we take log of this to make math easy, We pick the parameter vlues that maximise this likelihhood and is called maximum log likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,  y_train, y_test = train_test_split(X,y, random_state = 44, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## My model\n",
    "regr = ElasticNetCV(cv=5, \n",
    "                    random_state=0,\n",
    "                    l1_ratio=1,  ## this meaning it is lasso(L1) only\n",
    "                    verbose= 1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "             l1_ratio=1, max_iter=1000, n_alphas=100, n_jobs=None,\n",
       "             normalize=False, positive=False, precompute='auto', random_state=0,\n",
       "             selection='cyclic', tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6812090971473385\n",
      "41.36023201769211\n",
      "[17.85674926 21.74650639 22.32888526 33.06199197 15.26393185 31.70628932\n",
      " 21.17256413 24.93756341 29.87175455 26.28423564 21.46497731 24.89315182\n",
      " 20.43618809 26.39924334 29.24804063  7.39550079 19.97449656 23.50679272\n",
      " 19.69557535 20.1440684  29.59895593 27.53341218 34.93229704 24.33399196\n",
      " 13.32095042 19.91435254 -3.45736782 17.8061489  18.83351705 33.40708218\n",
      " 21.29488408 23.77886365 20.11553211 18.85102956 23.51928466 20.65525847\n",
      " 16.42271347 33.1307904  29.10022223 28.13579484 20.83726185 17.89494623\n",
      " 31.90844867 23.22409895 17.19632893 13.2959056  35.55340858 30.84508799\n",
      "  7.62933868 17.60656937 30.21113193 23.15211849 16.87582066 14.54271943\n",
      " 28.09472118 17.2897326  34.03518279 38.71005747 13.77672811 20.86271269\n",
      " 20.58401999 16.00771349 15.90815377 22.57876388 26.32761202 21.89544713\n",
      " 27.64504767 25.17555981 33.14675736 27.3516999  17.9292011  11.85539838\n",
      " 34.63465554 21.29949364 20.29116777 22.3736894  18.00568622 35.71740887\n",
      " 27.14875304  7.25552825 21.45223656 21.2043306  20.56189489 32.30024667\n",
      " 18.69729331 19.99712079 19.47645089 25.04139842 27.6002191   6.34763729\n",
      " 25.37236351 18.97363802 25.70642973 13.85707225 21.85342785 24.85036691\n",
      " 39.6156355  25.3134137  16.02626553 26.98093054 29.50871236 24.97907969]\n"
     ]
    }
   ],
   "source": [
    "print(regr.alpha_)\n",
    "\n",
    "print(regr.intercept_)\n",
    "\n",
    "print(regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error  3.592043739763328\n",
      "mean squared error  24.038989490965896\n",
      "R2 score  0.71540574604873\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "print(\"mean absolute error \",mean_absolute_error(y_test, regr.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"mean squared error \",mean_squared_error(y_test, regr.predict(X_test)))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2 score \",r2_score(y_test, regr.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1]\n",
    "## alpha zero meaning there is no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:0.0001, R2:0.75, MSE:20.92, RMSE:4.57\n",
      "Alpha:0.0010, R2:0.75, MSE:20.81, RMSE:4.56\n",
      "Alpha:0.0100, R2:0.75, MSE:20.92, RMSE:4.57\n",
      "Alpha:0.1000, R2:0.74, MSE:21.86, RMSE:4.68\n",
      "Alpha:0.3000, R2:0.73, MSE:23.08, RMSE:4.80\n",
      "Alpha:0.5000, R2:0.72, MSE:23.97, RMSE:4.90\n",
      "Alpha:0.7000, R2:0.71, MSE:24.65, RMSE:4.96\n",
      "Alpha:1.0000, R2:0.70, MSE:25.42, RMSE:5.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "import numpy as np\n",
    "for a in alphas:\n",
    "    model = ElasticNet(alpha=a).fit(X_train, y_train)   \n",
    "    score = model.score(X_test, y_test)\n",
    "    pred_y = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, pred_y)   \n",
    "    print(\"Alpha:{0:.4f}, R2:{1:.2f}, MSE:{2:.2f}, RMSE:{3:.2f}\"\n",
    "       .format(a, score, mse, np.sqrt(mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, as alpha becomes very small, n_iter must be increased to compensate for the slow learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
